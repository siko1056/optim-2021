{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f8ac141-c662-4df3-9da7-a4ca6042449e",
   "metadata": {},
   "source": [
    "# Unconstrained minimization algorithms\n",
    "\n",
    "Consider the following quadratic unconstrained problem\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "\\textrm{minimize}   & f(x) = \\frac{1}{2}x^{T}Ax + b^{T}x + c & \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $A$ is a symmetrix $n \\times n$ matrix, $x,b \\in \\mathbb{R}^{n}$, and $c \\in \\mathbb{R}$.\n",
    "\n",
    "$f$ is a $C^{\\infty}$-function with\n",
    "\n",
    "$$\n",
    "\\nabla f(x) = Ax + b \\quad\\text{and}\\quad \\nabla^{2} f(x) = A.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d9d858-e1be-4209-adad-36b57ee4bde9",
   "metadata": {},
   "source": [
    "- $f$ is convex $\\iff A \\succeq 0$ (positive semi-definite).\n",
    "  - **Local minima are global for convex functions!**\n",
    "- $f$ is strict convex $\\iff A \\succ 0$ (positive definite).\n",
    "  - $A$ invertible and $x^{*} = -A^{-1}b$ is the **unique global minimum**.\n",
    "- $A \\nsucceq 0 \\implies f$ has no local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e09e83-1659-4f7d-b425-97089477aae7",
   "metadata": {},
   "source": [
    "## Gradient methods\n",
    "\n",
    "The general idea of gradient methods is\n",
    "to find the minimum of an optimization problem of the form\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "\\textrm{minimize} & f(x), & \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "with $x \\in \\mathbb{R}^{n}$.\n",
    "\n",
    "Many common solution strategies\n",
    "choose from a **starting point** $x^{0} \\in \\mathbb{R}^{n}$\n",
    "in each step $k = 0, 1, \\ldots$ a scaled **descent direction** $\\alpha^{k} d^{k}$\n",
    "with $d^{k} \\in \\mathbb{R}^{n}$\n",
    "and positive bounded **step size** $0 < \\alpha^{k} < \\delta$\n",
    "with $\\delta, \\alpha^{k} \\in \\mathbb{R}$,\n",
    "such that a new point\n",
    "\n",
    "$$\n",
    "x^{k + 1} = x^{k} + \\alpha^{k} d^{k}, \\quad k = 0, 1, \\ldots\n",
    "$$\n",
    "\n",
    "is smaller than the previous one,\n",
    "in other words:\n",
    "\n",
    "$$\n",
    "f(x^{k} + \\alpha^{k} d^{k}) < f(x^{k}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678eb5a4-6cc7-4baa-b56a-06c32c063ae6",
   "metadata": {},
   "source": [
    "According to **Taylor's theorem**,\n",
    "close to a point $x^{k} \\in \\mathbb{R}^{n}$\n",
    "a function $f \\colon \\mathbb{R}^{n} \\to \\mathbb{R}$\n",
    "can be locally approximated by a linear\n",
    "\n",
    "$$\n",
    "\\tilde{f}(x^{k} + \\alpha^{k} d^{k}) := f(x^{k}) + \\nabla f(x^{k})^{T}(\\alpha^{k} d^{k})\n",
    "$$\n",
    "\n",
    "or quadratic function\n",
    "\n",
    "$$\n",
    "\\tilde{f}(x^{k} + \\alpha^{k} d^{k}) := f(x^{k}) + \\nabla f(x^{k})^{T}(\\alpha^{k} d^{k})\n",
    "  + \\frac{1}{2}(\\alpha^{k} d^{k})^{T}\\nabla^{2} f(x^{k})(\\alpha^{k} d^{k}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c025c03e-7d9e-4afb-a5f9-1acfd0bc4cd8",
   "metadata": {},
   "source": [
    "From the second term of the linar Taylor approximation,\n",
    "the property $f(x^{k} + \\alpha^{k} d^{k}) < f(x^{k})$\n",
    "can only be fulfilled, if $d^{k}$ is a **decent direction**:\n",
    "\n",
    "$$\n",
    "\\nabla f(x^{k})^{T}d^{k} < 0.\n",
    "$$\n",
    "\n",
    ":::{note}\n",
    "\n",
    "The most obvious choice for such a decent direction\n",
    "is $d^{k} := -\\nabla f(x^{k})$.\n",
    "The repetitive application of this method is called\n",
    "**gradient descent** or **steepest decent**.\n",
    "\n",
    "$$\n",
    "x^{k + 1} = x^{k} - \\alpha^{k} \\nabla f(x^{k}), \\quad k = 0, 1, \\ldots\n",
    "$$\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aabdf4-aefe-4c15-96f8-389fbe1582f9",
   "metadata": {},
   "source": [
    "A more sophisticated choice for the decent direction $d^{k}$\n",
    "can be obtained from the quadratic Taylor approximation.\n",
    "\n",
    "Like in the example from the introduction\n",
    "this is a quadratic unconstrained problem\n",
    "and it has a unique global minimum,\n",
    "if the Hessian matrix is positive definte.\n",
    "\n",
    "Thus deriving the quadratic Taylor approximation to $d^{k}$\n",
    "choosing $\\alpha^{k} = 1$, one obtains\n",
    "\n",
    "$$\n",
    "\\nabla \\tilde{f}(x^{k} + d^{k}) := \\nabla f(x^{k}) + \\nabla^{2} f(x^{k}) d^{k}.\n",
    "$$\n",
    "\n",
    "Finally, the decent direction $d^{k}$ is the solution of the following\n",
    "linear system of equations:\n",
    "\n",
    "$$\n",
    "\\nabla^{2} f(x^{k}) d^{k} = -\\nabla f(x^{k})\n",
    "$$\n",
    "\n",
    "The repetitive application of this method is called **Newton's method**:\n",
    "\n",
    "$$\n",
    "x^{k + 1} = x^{k} - \\alpha^{k} (\\nabla^{2} f(x^{k}))^{-1} \\nabla f(x^{k}), \\quad k = 0, 1, \\ldots\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f07f0f3-857b-4406-96f3-9be8ef060b67",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "\n",
    "So far the **step size** can be assumed as $\\alpha^{k} = 1$.\n",
    "\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "6.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
